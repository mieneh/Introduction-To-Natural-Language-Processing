{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnpCvhFcfD3Y",
        "outputId": "717ad831-b131-4ee8-ced4-fbdfdb0cd7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Đảm bảo bạn đã tải về các gói dữ liệu của NLTK\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Tải dữ liệu từ Brown Corpus với nhãn từ loại theo bộ nhãn Universal Tagset\n",
        "sentences = brown.tagged_sents(tagset='universal')[:1000]\n",
        "#print(len(sentences))\n",
        "#sentences = sentences[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8L9UDRwgHQJ",
        "outputId": "86e540c7-de7e-4bb5-b706-f4f7ffa99397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Định nghĩa hàm để trích xuất đặc trưng từ mỗi từ và ngữ cảnh của nó\n",
        "def features(sentence, index):\n",
        "    \"\"\" Trả về một dict mô tả các đặc trưng của từ và ngữ cảnh của nó \"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word_is_one_char': len(sentence[index - 1]) == 1 if index > 0 else False,\n",
        "    }"
      ],
      "metadata": {
        "id": "SIGlU6NSfIqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo dữ liệu đặc trưng và nhãn\n",
        "X, y = [], []\n",
        "for sent in sentences:\n",
        "    untagged = [word for word, tag in sent]\n",
        "    for i, (word, tag) in enumerate(sent):\n",
        "        X.append(features(untagged, i))\n",
        "        y.append(tag)\n"
      ],
      "metadata": {
        "id": "kNGB-IZauHvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tạo pipeline bao gồm vector hóa và mô hình Logistic Regression\n",
        "#model = make_pipeline(DictVectorizer(sparse=False), LogisticRegression(max_iter=10))\n",
        "\n",
        "# Bước 1: Vector hóa dữ liệu\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "X_train_transformed = vectorizer.fit_transform(X_train)\n",
        "X_test_transformed = vectorizer.transform(X_test)\n",
        "print(X_train_transformed.shape)\n",
        "\n",
        "# Bước 2: Huấn luyện mô hình Logistic Regression\n",
        "model = LogisticRegression(max_iter=10)\n",
        "model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "model.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Đánh giá mô hình\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iWK61IJsfTdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8c8b1e-13e1-4906-e5c2-6f85e3a63337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17663, 15730)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8922101449275363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga-K9xf4BYTQ",
        "outputId": "76d3693e-71e1-41ee-f267-41bac971cc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'word': 'The', 'is_first': True, 'is_last': False, 'prev_word': '', 'next_word': 'Fulton', 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'Th', 'prefix-3': 'The', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'The', 'prev_word_is_one_char': False}\n"
          ]
        }
      ]
    }
  ]
}